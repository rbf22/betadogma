# BetaDogma

> _‚ÄúRevising the Central Dogma through data.‚Äù_  
> From DNA to RNA to (in)stability ‚Äî a unified model for transcript structure, abundance, and NMD fate.

---

## üî¨ Overview

**BetaDogma** is a research framework that learns the **probabilistic central dogma** of molecular biology.  
It takes raw **genomic sequence** (¬± variants) and predicts:

- **Dominant mRNA isoform** ‚Äî inferred from splice, TSS, and polyA patterns  
- **Relative isoform abundance (œà)** ‚Äî learned from RNA-seq junction data  
- **Nonsense-mediated decay (NMD) probability** ‚Äî combining rule-based and learned features  
- **Variant effects** ‚Äî Œîœà and ŒîNMD due to sequence edits

The project fine-tunes a long-context genomic language model (e.g. **GENERator**) with specialized biological heads and interpretable outputs.

---

## üß± Architecture

```
Genomic sequence (+ optional variants)
       ‚îÇ
       ‚ñº
   BetaDogma backbone (GENERator)
       ‚îÇ
Base embeddings (nucleotide resolution)
       ‚îÇ
‚îú‚îÄ‚îÄ splice_head ‚Üí donor/acceptor logits
‚îú‚îÄ‚îÄ tss_head ‚Üí transcription start sites
‚îú‚îÄ‚îÄ polya_head ‚Üí cleavage / 3‚Ä≤ ends
‚îú‚îÄ‚îÄ orf_head ‚Üí CDS start/stop/frame
‚îî‚îÄ‚îÄ variant_channel ‚Üí REF/ALT effects
       ‚îÇ
Isoform decoder + œà head
       ‚îÇ
Dominant transcript selector
       ‚îÇ
NMD head (rule + learned features)
       ‚ñº
Final outputs: mRNA structure + P(NMD)
```

---

## üß© Core Modules

| Module | Function |
|---------|-----------|
| `core/` | Backbone + per-base heads (splice, TSS, polyA, ORF) |
| `decoder/` | Isoform graph assembly and œà scoring |
| `nmd/` | Rule-augmented classifier for transcript decay |
| `variant/` | Variant encoding, synthetic mutagenesis, Œî computations |
| `data/` | Data ingestion and preprocessing pipelines |
| `experiments/` | Training configurations, checkpoints |
| `notebooks/` | Analysis, visualization, evaluation tools |

---

## üìö Data Layers

Detailed documentation: [`docs/DATASETS.md`](./docs/DATASETS.md)

| Purpose | Dataset |
|----------|----------|
| Gene structure | GENCODE, RefSeq |
| Isoform abundance | GTEx, ENCODE |
| Long-read truth | PacBio Iso-Seq, Nanopore |
| TSS / polyA sites | FANTOM5, PolyA-DB |
| Translation frame | Ribo-seq |
| NMD labels | UPF1/SMG6 knockdown RNA-seq, 4sU-seq |
| Variant effects | GTEx eQTL/sQTL, gnomAD, MPRA reporters |

---

## üß† Training Phases

1. **Structural fine-tuning** ‚Äì teach splicing, TSS, and polyA recognition.  
2. **Isoform decoding** ‚Äì learn exon chains and œà distribution.  
3. **NMD prediction** ‚Äì hybrid rule + learned classifier.  
4. **Variant adaptation** ‚Äì train for Œîœà and ŒîNMD sensitivity.  
5. **Joint optimization** ‚Äì multi-task fine-tuning end-to-end.

---

## ‚öôÔ∏è Quickstart (conceptual)

> **Note:** This example is a conceptual guide. The API is under active development and this code is not yet runnable.

```python
from betadogma import BetaDogmaModel, preprocess_sequence, preprocess_variant

model = BetaDogmaModel.from_pretrained("betadogma/generator-base")

seq = preprocess_sequence(chrom="chr17", start=43044294, end=43099294)
variant = preprocess_variant("17:43051000A>T")

out_ref = model.predict(seq)
out_alt = model.predict(seq, variant=variant)

print(out_ref.dominant_isoform)
print("ŒîNMD =", out_alt.P_NMD - out_ref.P_NMD)
```

---

## üß™ Evaluation

| Metric | Description |
|---------|-------------|
| **Splice F1 / junction accuracy** | donor/acceptor prediction |
| **Isoform correctness** | exon chain match |
| **œà correlation** | usage prediction vs. RNA-seq |
| **NMD AUROC / AUPRC** | decay classification |
| **Œîœà / ŒîNMD correlation** | variant effect prediction |

---

## üß¨ Philosophy

> The ‚Äúcentral dogma‚Äù was never static ‚Äî transcription and translation are dynamic systems.  
> **BetaDogma** re-learns these principles directly from data, modeling uncertainty, regulation, and decay as emergent behaviors.

---

## üìñ Docs

- [`docs/DATASETS.md`](./docs/DATASETS.md)
- [`docs/MODEL_CARD.md`](./docs/MODEL_CARD.md)
- [`docs/TASKS.md`](./docs/TASKS.md)
- [`CONTRIBUTING.md`](./CONTRIBUTING.md)
- [`LICENSE`](./LICENSE)

---

## üõ†Ô∏è Data Processing

### 1. Fetching Raw Data (`data/fetch_data.py`)

This script downloads and verifies the raw data needed for training.

#### Usage:
```bash
# Download all data (full dataset)
poetry run python data/fetch_data.py

# Check download status without downloading
poetry run python data/fetch_data.py --check

# Force re-download of specific files
poetry run python data/fetch_data.py --force gtex_junctions
```

#### Features:
- Downloads data from various sources (GTEx, GENCODE, etc.)
- Verifies file integrity with checksums
- Skips already downloaded files by default
- Supports resuming interrupted downloads

### 2. Preparing Training Data (`train/make_training_data.py`)

Processes raw data into training-ready format with PSI calculations and gene annotations.

#### Basic Usage:
```bash
# Full processing (all steps)
poetry run python train/make_training_data.py --config train/configs/data.base.yaml
```

#### Smoke Test Mode (for quick validation):
```bash
# Process only a small subset of data
poetry run python train/make_training_data.py --config train/configs/data.base.yaml --smoke
```

#### Checkpointing and Resuming:
```bash
# Resume from a specific step if interrupted
poetry run python train/make_training_data.py --config train/configs/data.base.yaml --from-step gtex

# Use a custom checkpoint directory
poetry run python train/make_training_data.py --config train/configs/data.base.yaml --checkpoint-dir my_checkpoints
```

#### Available Steps:
- `gencode`: Process GENCODE annotations
- `gtex`: Process GTEx junction data
- `variants`: Process variant data
- `data`: Final data aggregation

## üìú License

MIT
