experiment: betadogma_baseline
seed: 42

data:
  path: data/processed      # ‚Üê point this to your prepared data
  num_workers: 8
  batch_size: 32
  input_dim: 1024           # only used by fallback ToyDataset

model:
  hidden_dim: 512
  dropout: 0.1
  # add any kwargs your betadogma.decoder.DecoderModel expects

optim:
  lr: 3e-4
  weight_decay: 0.01
  warmup_steps: 1000
  epochs: 20

train:
  precision: bf16           # or "16-mixed" if you prefer
  accumulate_grad_batches: 2
  val_every_n_steps: 500
  early_stopping_patience: 5

logging:
  logdir: runs/betadogma_baseline

ckpt:
  dir: checkpoints/betadogma_baseline
  save_top_k: 2
  monitor: val/score
  mode: max
