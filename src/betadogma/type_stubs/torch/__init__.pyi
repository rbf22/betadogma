from typing import Any, Dict, List, Optional, Tuple, Union, overload, TypeVar, Generic, Sequence, Type, Callable
import numpy as np

T = TypeVar('T')
Tensor = Any

class Tensor(Generic[T]):
    def __init__(self, *args: Any, **kwargs: Any) -> None: ...
    def to(self, *args: Any, **kwargs: Any) -> 'Tensor': ...
    def cuda(self, device: Any = None, non_blocking: bool = False) -> 'Tensor': ...
    def cpu(self) -> 'Tensor': ...
    def float(self) -> 'Tensor': ...
    def long(self) -> 'Tensor': ...
    def int(self) -> 'Tensor': ...
    def bool(self) -> 'Tensor': ...
    def shape(self) -> Tuple[int, ...]: ...
    def size(self, dim: Optional[int] = None) -> Union[Tuple[int, ...], int]: ...
    def dim(self) -> int: ...
    def numel(self) -> int: ...
    def view(self, *shape: int) -> 'Tensor': ...
    def reshape(self, *shape: int) -> 'Tensor': ...
    def permute(self, *dims: int) -> 'Tensor': ...
    def transpose(self, dim0: int, dim1: int) -> 'Tensor': ...
    def unsqueeze(self, dim: int) -> 'Tensor': ...
    def squeeze(self, dim: Optional[int] = None) -> 'Tensor': ...
    def expand(self, *sizes: int) -> 'Tensor': ...
    def expand_as(self, other: 'Tensor') -> 'Tensor': ...
    def repeat(self, *sizes: int) -> 'Tensor': ...
    def clone(self) -> 'Tensor': ...
    def detach(self) -> 'Tensor': ...
    def requires_grad_(self, requires_grad: bool = True) -> 'Tensor': ...
    def backward(self, gradient: Optional['Tensor'] = None, retain_graph: Any = None, create_graph: bool = False) -> None: ...
    def item(self) -> Any: ...
    def numpy(self) -> np.ndarray: ...
    def cpu(self) -> 'Tensor': ...
    def cuda(self, device: Any = None, non_blocking: bool = False) -> 'Tensor': ...
    def tolist(self) -> List: ...
    def __add__(self, other: Any) -> 'Tensor': ...
    def __sub__(self, other: Any) -> 'Tensor': ...
    def __mul__(self, other: Any) -> 'Tensor': ...
    def __truediv__(self, other: Any) -> 'Tensor': ...
    def __floordiv__(self, other: Any) -> 'Tensor': ...
    def __mod__(self, other: Any) -> 'Tensor': ...
    def __pow__(self, other: Any) -> 'Tensor': ...
    def __neg__(self) -> 'Tensor': ...
    def __pos__(self) -> 'Tensor': ...
    def __abs__(self) -> 'Tensor': ...
    def __invert__(self) -> 'Tensor': ...
    def __and__(self, other: Any) -> 'Tensor': ...
    def __or__(self, other: Any) -> 'Tensor': ...
    def __xor__(self, other: Any) -> 'Tensor': ...
    def __lt__(self, other: Any) -> 'Tensor': ...
    def __le__(self, other: Any) -> 'Tensor': ...
    def __gt__(self, other: Any) -> 'Tensor': ...
    def __ge__(self, other: Any) -> 'Tensor': ...
    def __eq__(self, other: Any) -> 'Tensor': ...
    def __ne__(self, other: Any) -> 'Tensor': ...
    def __getitem__(self, key: Any) -> 'Tensor': ...
    def __setitem__(self, key: Any, value: Any) -> None: ...
    def __len__(self) -> int: ...
    def __iter__(self): ...

class nn:
    class Module:
        def __init__(self) -> None: ...
        def forward(self, *input: Any) -> Any: ...
        def __call__(self, *input: Any, **kwargs: Any) -> Any: ...
        def to(self, *args: Any, **kwargs: Any) -> 'nn.Module': ...
        def cuda(self, device: Any = None) -> 'nn.Module': ...
        def cpu(self) -> 'nn.Module': ...
        def train(self, mode: bool = True) -> 'nn.Module': ...
        def eval(self) -> 'nn.Module': ...
        def parameters(self, recurse: bool = True) -> Any: ...
        def named_parameters(self, prefix: str = '', recurse: bool = True) -> Any: ...
        def children(self) -> Any: ...
        def named_children(self) -> Any: ...
        def modules(self) -> Any: ...
        def named_modules(self, memo: Any = None, prefix: str = '') -> Any: ...
        def apply(self, fn: Any) -> 'nn.Module': ...
        def register_buffer(self, name: str, tensor: Tensor, persistent: bool = True) -> None: ...
        def register_parameter(self, name: str, param: Any) -> None: ...
        def add_module(self, name: str, module: Optional['nn.Module']) -> None: ...
        def __getattr__(self, name: str) -> Any: ...
        def __setattr__(self, name: str, value: Any) -> None: ...
        def __delattr__(self, name: str) -> None: ...
        def __repr__(self) -> str: ...

    class Linear(nn.Module):
        def __init__(self, in_features: int, out_features: int, bias: bool = True) -> None: ...
        def forward(self, input: Tensor) -> Tensor: ...

    class Conv1d(nn.Module):
        def __init__(
            self, 
            in_channels: int, 
            out_channels: int, 
            kernel_size: int, 
            stride: int = 1, 
            padding: int = 0, 
            dilation: int = 1, 
            groups: int = 1, 
            bias: bool = True, 
            padding_mode: str = 'zeros'
        ) -> None: ...
        def forward(self, input: Tensor) -> Tensor: ...

    class LayerNorm(nn.Module):
        def __init__(
            self, 
            normalized_shape: int, 
            eps: float = 1e-5, 
            elementwise_affine: bool = True
        ) -> None: ...
        def forward(self, input: Tensor) -> Tensor: ...

    class Dropout(nn.Module):
        def __init__(self, p: float = 0.5, inplace: bool = False) -> None: ...
        def forward(self, input: Tensor) -> Tensor: ...

    class Sequential(nn.Module):
        def __init__(self, *args: nn.Module) -> None: ...
        def forward(self, input: Tensor) -> Tensor: ...
        def __getitem__(self, idx: int) -> nn.Module: ...
        def __len__(self) -> int: ...

    class ModuleDict(nn.Module):
        def __init__(self, modules: Optional[Dict[str, nn.Module]] = None) -> None: ...
        def __getitem__(self, key: str) -> nn.Module: ...
        def __setitem__(self, key: str, module: nn.Module) -> None: ...
        def __delitem__(self, key: str) -> None: ...
        def __len__(self) -> int: ...
        def __iter__(self): ...
        def keys(self) -> List[str]: ...
        def items(self) -> List[Tuple[str, nn.Module]]: ...
        def values(self) -> List[nn.Module]: ...

    class GELU(nn.Module):
        def forward(self, input: Tensor) -> Tensor: ...

    class MSELoss(nn.Module):
        def __init__(self, reduction: str = 'mean') -> None: ...
        def forward(self, input: Tensor, target: Tensor) -> Tensor: ...

    class CrossEntropyLoss(nn.Module):
        def __init__(
            self, 
            weight: Optional[Tensor] = None, 
            ignore_index: int = -100, 
            reduction: str = 'mean'
        ) -> None: ...
        def forward(self, input: Tensor, target: Tensor) -> Tensor: ...

    class BCEWithLogitsLoss(nn.Module):
        def __init__(
            self, 
            weight: Optional[Tensor] = None, 
            reduction: str = 'mean',
            pos_weight: Optional[Tensor] = None
        ) -> None: ...
        def forward(self, input: Tensor, target: Tensor) -> Tensor: ...

def tensor(
    data: Any,
    dtype: Any = None,
    device: Any = None,
    requires_grad: bool = False
) -> Tensor: ...

def zeros(
    *size: int,
    out: Optional[Tensor] = None,
    dtype: Any = None,
    layout: Any = None,
    device: Any = None,
    requires_grad: bool = False
) -> Tensor: ...

def ones(
    *size: int,
    out: Optional[Tensor] = None,
    dtype: Any = None,
    layout: Any = None,
    device: Any = None,
    requires_grad: bool = False
) -> Tensor: ...

def arange(
    start: int,
    end: Optional[int] = None,
    step: int = 1,
    out: Optional[Tensor] = None,
    dtype: Any = None,
    layout: Any = None,
    device: Any = None,
    requires_grad: bool = False
) -> Tensor: ...

def cat(
    tensors: Sequence[Tensor],
    dim: int = 0,
    out: Optional[Tensor] = None
) -> Tensor: ...

def stack(
    tensors: Sequence[Tensor],
    dim: int = 0,
    out: Optional[Tensor] = None
) -> Tensor: ...

def no_grad() -> Any: ...

def manual_seed(seed: int) -> None: ...

def cuda_device_count() -> int: ...

def cuda_is_available() -> bool: ...

def cuda_current_device() -> int: ...

def cuda_set_device(device: int) -> None: ...

def save(obj: Any, f: Any) -> None: ...

def load(f: Any, map_location: Any = None) -> Any: ...

class optim:
    class Optimizer:
        def __init__(self, params: Any, defaults: Dict[str, Any]) -> None: ...
        def zero_grad(self, set_to_none: bool = False) -> None: ...
        def step(self, closure: Any = None) -> Optional[float]: ...
        def state_dict(self) -> Dict[str, Any]: ...
        def load_state_dict(self, state_dict: Dict[str, Any]) -> None: ...

    class Adam(Optimizer):
        def __init__(
            self,
            params: Any,
            lr: float = 1e-3,
            betas: Tuple[float, float] = (0.9, 0.999),
            eps: float = 1e-8,
            weight_decay: float = 0,
            amsgrad: bool = False
        ) -> None: ...

    class AdamW(Optimizer):
        def __init__(
            self,
            params: Any,
            lr: float = 1e-3,
            betas: Tuple[float, float] = (0.9, 0.999),
            eps: float = 1e-8,
            weight_decay: float = 1e-2,
            amsgrad: bool = False
        ) -> None: ...

    class SGD(Optimizer):
        def __init__(
            self,
            params: Any,
            lr: float,
            momentum: float = 0,
            dampening: float = 0,
            weight_decay: float = 0,
            nesterov: bool = False
        ) -> None: ...

def manual_seed(seed: int) -> None: ...

def cuda_is_available() -> bool: ...

def cuda_device_count() -> int: ...

def cuda_current_device() -> int: ...

def cuda_set_device(device: int) -> None: ...

def save(obj: Any, f: Any) -> None: ...

def load(f: Any, map_location: Any = None) -> Any: ...
